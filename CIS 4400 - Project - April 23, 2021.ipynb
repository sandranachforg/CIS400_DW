{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL - Data Transformation\n",
    "\n",
    "@by Sandra Nachforg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sandra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sandra/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sandra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy as tw\n",
    "import re\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "from preprocessor.api import clean, tokenize, parse\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import datetime as dt\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "from textblob import TextBlob\n",
    "\n",
    "import requests \n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We are interested in the sentiment of the following companies :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Netflix (hashtag: netflix) \\\n",
    "b) Coca Cola (hashtag: CocaCola) \\\n",
    "c) Tesla (hashtag: Tesla)\\\n",
    "d) Nike (hashtag: Nike) \\\n",
    "e) Apple (hashtag: Apple) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get data from Alpha Vantage (API that has company information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"\"\n",
    "URL= \"https://www.alphavantage.co/query?function=OVERVIEW&symbol=AAPL&apikey=key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_data(company,key):\n",
    "    \n",
    "    key = key\n",
    "    URL= \"https://www.alphavantage.co/query?function=OVERVIEW\"\n",
    "    r= requests.get(url= URL)\n",
    "    data = r.json()\n",
    "    df = json_normalize(data)   #transforms the data into a dataframe\n",
    "    df  = df[[\"Name\", \"Sector\", \"Industry\"]]   #returns only select columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Authorize Account for Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.twitter.com/oauth/authorize?oauth_token=gOK_sQAAAAABNQviAAABeP8xQbM\n",
      "What's the pin value? 3399798\n"
     ]
    }
   ],
   "source": [
    "consumer_key= ''\n",
    "consumer_secret= ''\n",
    "access_token= \"\"\n",
    "access_token_secret= ''\n",
    "callback_url = 'oob'\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret, callback_url)\n",
    "redirect_url= auth.get_authorization_url()\n",
    "\n",
    "print (redirect_url)\n",
    "\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "user_pin_input = input (\"What's the pin value? \")\n",
    "api = tw.API (auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get function to get data from Twitter API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes 3 parameters and helps us extract the data from twitter\n",
    "# The \"hashtag\" is the company name we are analyzing\n",
    "# The \"company_id\" is the id we assigned to that particular company in the company dimensions\n",
    "# The \"document_id\" is the id we assigned to source (Twitter  - 1, Forbes - 2 etc.) \n",
    "\n",
    "def get_tweets(hashtag, company_id, document_id):\n",
    "    \n",
    "    number_of_tweets = 100\n",
    "    tweets = []\n",
    "    likes= []\n",
    "    time= []\n",
    "    user = []\n",
    "    author = []\n",
    "    retweet=[]\n",
    "    \n",
    "    # q = pass the hashtags\n",
    "    \n",
    "    for i in tw.Cursor(api.search, q= hashtag, tweet_mode= 'extended', lang= \"en\").items(number_of_tweets):\n",
    "        tweets.append(i.full_text)\n",
    "        likes.append(i.favorite_count)\n",
    "        time.append(i.created_at)\n",
    "        retweet.append(i.retweet_count) \n",
    "      \n",
    "        \n",
    "    df= pd.DataFrame({'tweets': tweets, 'likes': likes, 'timestamp': time, 'retweet': retweet})\n",
    "    \n",
    "    df[\"document_id\"] = document_id\n",
    "    df[\"company_id\"] = company_id\n",
    "    \n",
    "    df= df[[\"company_id\", \"timestamp\", \"document_id\", \"tweets\", \"retweet\", \"likes\"]]\n",
    "    \n",
    "    df.rename(columns = {\"timestamp\": \"time_id\", \"tweets\": \"original_description\"}, inplace= True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create function to get data from News API - can also specify the publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Mohamed Abouregila\n",
    "get_headlines()\n",
    " this function takes company, fromDate, and to date in form of  \"yyyy-mm-dd\", and returns a dataframe of all \n",
    " headlines posted about this company in the specified period\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "'''Modified: add additional argument: company_id to differentiate from each other - Sandra Nachforg'''\n",
    "\n",
    "def get_headlines(company_id, company,fromDate,toDate,key):\n",
    "    \n",
    "    url= \"http://newsapi.org/v2/everything?\"\n",
    "    parameters = {\n",
    "            \"qInTitle\": company,\n",
    "            \"language\":\"en\",\n",
    "            \"from\":fromDate,\n",
    "            \"to\":toDate,\n",
    "            \"apiKey\":key,\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url,params=parameters)\n",
    "\n",
    "    df = response.json()\n",
    "    df = pd.DataFrame.from_dict(df)\n",
    "    df = pd.concat([df.drop(['articles'], axis=1), df['articles'].apply(pd.Series)], axis=1)\n",
    "    df [[\"Drop\",\"source\"]] = df.source.apply(pd.Series)\n",
    "    df [\"company_id\"]  = company_id\n",
    "    df [\"document_id\"] = 2\n",
    "    df [\"retweet_count\"] = \"\"\n",
    "    df [\"like_count\"] = \"\"\n",
    "    \n",
    "    df = df [[\"company_id\", \"publishedAt\", \"document_id\", \"title\", \"retweet_count\", \"like_count\"]]\n",
    "    \n",
    "    \n",
    "    df [\"publishedAt\"] = pd.to_datetime(df_news.publishedAt).dt.tz_localize(None)\n",
    "    \n",
    "    df.rename(columns = {\"publishedAt\": \"time_id\", \"title\": \"original_description\"}, inplace = True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Get tweets for the companies- Apply the \"get_tweets()\"-  function\n",
    "\n",
    "This function takes three arguments: hashtag (e.g. netflix), company_id (based on copmany dimension), document_id (based on document dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_twitter = get_tweets(\"netflix\", 1,1)    #netflix, 1 - company_id, 1- document_id \n",
    "tesla_twitter = get_tweets(\"tsla\", 2, 1) \n",
    "cocacola_twitter = get_tweets(\"cocacola\", 3,1)\n",
    "starbucks_twitter = get_tweets(\"starbucks\", 4, 1 )\n",
    "nike_twitter = get_tweets(\"nike\", 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Get headlines for the companies - Apply the get_headlines () - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_news = get_headlines (1, \"netflix\",'2021-04-21','2021-04-22', \"\" )\n",
    "tesla_news = get_headlines (2, \"tesla\", '2021-04-21','2021-04-22', \"\")\n",
    "#cocacola_news = get_headlines (3, \"cocacola\", '2021-04-21','2021-04-22', \"\") # didn't have source_ \n",
    "starbucks_news = get_headlines (4, \"starbucks\", '2021-04-21','2021-04-22', \"\")\n",
    "nike_news = get_headlines (5, \"nike\", '2021-04-21','2021-04-22', \"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Concat all dataframes (stack on top of each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table = pd.concat([netflix_twitter,tesla_twitter, cocacola_twitter, starbucks_twitter, nike_twitter,\n",
    "                        netflix_news, tesla_news, starbucks_news, nike_news], axis=0)   #stacks each dataframe on top of each other\n",
    "\n",
    "fact_table.to_csv(\"fact_table.csv\")  #saves fact_table --> in case we want to save a copy to our hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>original_description</th>\n",
       "      <th>retweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:32</td>\n",
       "      <td>1</td>\n",
       "      <td>Bro, why do we even have a huddle. This shit c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:32</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @thecartooncrave: Netflix will reportedly s...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Rocioceja_: The way y‚Äôall canceled so many...</td>\n",
       "      <td>483.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>1</td>\n",
       "      <td>@Smileyimxx ‡∏Ñ‡∏π‡∏°‡πÄ‡∏à‡∏ü‡∏ó‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÅ‡∏´‡∏•‡∏∞ 3&amp;gt;\\n\\nìÇã Netf...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @bonobochick: And when I say I have a list,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_id             time_id  document_id  \\\n",
       "0           1 2021-04-23 14:45:32            1   \n",
       "1           1 2021-04-23 14:45:32            1   \n",
       "2           1 2021-04-23 14:45:31            1   \n",
       "3           1 2021-04-23 14:45:31            1   \n",
       "4           1 2021-04-23 14:45:31            1   \n",
       "\n",
       "                                original_description  retweet  likes  \\\n",
       "0  Bro, why do we even have a huddle. This shit c...      0.0    0.0   \n",
       "1  RT @thecartooncrave: Netflix will reportedly s...     15.0    0.0   \n",
       "2  RT @Rocioceja_: The way y‚Äôall canceled so many...    483.0    0.0   \n",
       "3  @Smileyimxx ‡∏Ñ‡∏π‡∏°‡πÄ‡∏à‡∏ü‡∏ó‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÅ‡∏´‡∏•‡∏∞ 3&gt;\\n\\nìÇã Netf...      0.0    0.0   \n",
       "4  RT @bonobochick: And when I say I have a list,...      1.0    0.0   \n",
       "\n",
       "  retweet_count like_count  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Create function to clean original_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to do this to do sentiment analysis on the headlines/tweets later\n",
    "\n",
    "def cleanup_description(s):\n",
    "    text = s.lower().split()       # makes all characters lower case\n",
    "    text = list(filter(lambda x: \"http\" not in x, text))        # removes http\n",
    "    text = list(filter(lambda x: not x.startswith(\"@\"), text))   #removes the @\n",
    "    text = list(map(lambda x: emoji.demojize(x, delimiters=(\"\", \",\")).replace(\"_\", \" \"), text))  # replaces emojis with word\n",
    "    text = list(map(lambda x: re.sub(\"[^a-zA-Z ]+\", \"\", x), text)) \n",
    "    #only keeps letter from a-z and A-Z\n",
    "    #text = [word for word in text if not word in stopwords.words()]     #removes stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = list(map(lambda x: lemmatizer.lemmatize(x), text))     #lemmatizes the text\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Create a function that calculates the sentiment score for each clean_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(df):\n",
    "    \n",
    "    df[\"cleaned_description\"] =  df[\"original_description\"].apply(lambda x: cleanup_description(x))\n",
    "    df[[\"polarity\", \"subjectivity\"]] = df[\"cleaned_description\"].apply(lambda x: pd.Series(TextBlob(x).sentiment))\n",
    "\n",
    "    for index, row in df[\"cleaned_description\"].iteritems():\n",
    "        score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    \n",
    "        neg = score[\"neg\"]\n",
    "        neu = score[\"neu\"]\n",
    "        pos = score[\"pos\"]\n",
    "        comp = score[\"compound\"]\n",
    "    \n",
    "        if neg > pos:\n",
    "            df.loc[index, \"sentiment\"] = \"negative\"\n",
    "        elif pos > neg:\n",
    "            df.loc[index, \"sentiment\"] = \"positive\"\n",
    "        else:\n",
    "            df.loc[index, \"sentiment\"] = \"neutral\"\n",
    "    \n",
    "    df.rename(columns = {\"timestamp\": \"time_id\", \"tweets\": \"original_description\", \"cleaned_tweets\":\"cleaned_description\", \n",
    "                        \"retweet\": \"retweet_count\", \"likes\": \"like_count\"}, inplace = True)\n",
    "    \n",
    "    df = df[[\"company_id\", \"time_id\", \"document_id\", \"original_description\",\"cleaned_description\", \"retweet_count\", \n",
    "            \"like_count\", \"polarity\", \"subjectivity\", \"sentiment\"]]\n",
    "    \n",
    "    \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. After we have our finished dataframe, we can apply  the \"get_sentiment_score\"- function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>original_description</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:32</td>\n",
       "      <td>1</td>\n",
       "      <td>Bro, why do we even have a huddle. This shit c...</td>\n",
       "      <td>bro why do we even have a huddle this shit cou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.187879</td>\n",
       "      <td>0.751515</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:32</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @thecartooncrave: Netflix will reportedly s...</td>\n",
       "      <td>rt netflix will reportedly spend  billion on c...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Rocioceja_: The way y‚Äôall canceled so many...</td>\n",
       "      <td>rt the way yall canceled so many good as netfl...</td>\n",
       "      <td>483.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>1</td>\n",
       "      <td>@Smileyimxx ‡∏Ñ‡∏π‡∏°‡πÄ‡∏à‡∏ü‡∏ó‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÅ‡∏´‡∏•‡∏∞ 3&amp;gt;\\n\\nìÇã Netf...</td>\n",
       "      <td>gt  netflix or dinner  dinner mouse trap kiss...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @bonobochick: And when I say I have a list,...</td>\n",
       "      <td>rt and when i say i have a list netflix show t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-22 23:55:45</td>\n",
       "      <td>2</td>\n",
       "      <td>atmos' Nike LeBron 18 Low \"Sakura\" Nods to Jap...</td>\n",
       "      <td>atmos nike lebron  low sakura nod to japan che...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-22 23:56:03</td>\n",
       "      <td>2</td>\n",
       "      <td>Erik Arteaga Announces Vyzer LA Sponsorship Sk...</td>\n",
       "      <td>erik arteaga announces vyzer la sponsorship sk...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-21 11:27:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Take an Official Look At the Nike SB Dunk High...</td>\n",
       "      <td>take an official look at the nike sb dunk high...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-21 22:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Nike &amp; Urbanstar Team Up With The Silhouettes ...</td>\n",
       "      <td>nike  urbanstar team up with the silhouette pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-21 18:27:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Nike Refurbished Gets More Mileage Out Of Used...</td>\n",
       "      <td>nike refurbished get more mileage out of used ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    company_id             time_id  document_id  \\\n",
       "0            1 2021-04-23 14:45:32            1   \n",
       "1            1 2021-04-23 14:45:32            1   \n",
       "2            1 2021-04-23 14:45:31            1   \n",
       "3            1 2021-04-23 14:45:31            1   \n",
       "4            1 2021-04-23 14:45:31            1   \n",
       "..         ...                 ...          ...   \n",
       "15           5 2021-04-22 23:55:45            2   \n",
       "16           5 2021-04-22 23:56:03            2   \n",
       "17           5 2021-04-21 11:27:00            2   \n",
       "18           5 2021-04-21 22:00:00            2   \n",
       "19           5 2021-04-21 18:27:00            2   \n",
       "\n",
       "                                 original_description  \\\n",
       "0   Bro, why do we even have a huddle. This shit c...   \n",
       "1   RT @thecartooncrave: Netflix will reportedly s...   \n",
       "2   RT @Rocioceja_: The way y‚Äôall canceled so many...   \n",
       "3   @Smileyimxx ‡∏Ñ‡∏π‡∏°‡πÄ‡∏à‡∏ü‡∏ó‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÅ‡∏´‡∏•‡∏∞ 3&gt;\\n\\nìÇã Netf...   \n",
       "4   RT @bonobochick: And when I say I have a list,...   \n",
       "..                                                ...   \n",
       "15  atmos' Nike LeBron 18 Low \"Sakura\" Nods to Jap...   \n",
       "16  Erik Arteaga Announces Vyzer LA Sponsorship Sk...   \n",
       "17  Take an Official Look At the Nike SB Dunk High...   \n",
       "18  Nike & Urbanstar Team Up With The Silhouettes ...   \n",
       "19  Nike Refurbished Gets More Mileage Out Of Used...   \n",
       "\n",
       "                                  cleaned_description retweet_count  \\\n",
       "0   bro why do we even have a huddle this shit cou...           0.0   \n",
       "1   rt netflix will reportedly spend  billion on c...          15.0   \n",
       "2   rt the way yall canceled so many good as netfl...         483.0   \n",
       "3    gt  netflix or dinner  dinner mouse trap kiss...           0.0   \n",
       "4   rt and when i say i have a list netflix show t...           1.0   \n",
       "..                                                ...           ...   \n",
       "15  atmos nike lebron  low sakura nod to japan che...           NaN   \n",
       "16  erik arteaga announces vyzer la sponsorship sk...           NaN   \n",
       "17  take an official look at the nike sb dunk high...           NaN   \n",
       "18  nike  urbanstar team up with the silhouette pr...           NaN   \n",
       "19  nike refurbished get more mileage out of used ...           NaN   \n",
       "\n",
       "   retweet_count like_count like_count  polarity  subjectivity sentiment  \n",
       "0            NaN        0.0        NaN -0.187879      0.751515   neutral  \n",
       "1            NaN        0.0        NaN  0.000000      0.000000  positive  \n",
       "2            NaN        0.0        NaN  0.300000      0.525000  negative  \n",
       "3            NaN        0.0        NaN  0.000000      0.000000   neutral  \n",
       "4            NaN        0.0        NaN  0.000000      0.000000  positive  \n",
       "..           ...        ...        ...       ...           ...       ...  \n",
       "15                      NaN             0.000000      0.300000  negative  \n",
       "16                      NaN             0.000000      0.000000   neutral  \n",
       "17                      NaN             0.160000      0.540000   neutral  \n",
       "18                      NaN             0.000000      0.000000   neutral  \n",
       "19                      NaN             0.500000      0.500000   neutral  \n",
       "\n",
       "[571 rows x 12 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_score(fact_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Lastly - we create time dimension by getting all unique timestamps from fact table and then transform it  \n",
    "@ by Sandra Nachforg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dim(df):\n",
    "    df_time= df[[\"time_id\"]]\n",
    "    df_time['Time'] = pd.to_datetime(df['time_id']).dt.time\n",
    "    df_time['Day']= pd.to_datetime(df['time_id']).dt.day\n",
    "    df_time['Month']= pd.to_datetime(df['time_id']).dt.month\n",
    "    df_time['Year']= pd.to_datetime(df['time_id']).dt.year\n",
    "    \n",
    "    df_time.drop_duplicates(inplace = True)\n",
    "    \n",
    "    return df_time\n",
    "\n",
    "## timestamp is the primary key here in SQL\n",
    "## In fact table it is part of the primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-a8e35094c473>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_time['Time'] = pd.to_datetime(df['time_id']).dt.time\n",
      "<ipython-input-139-a8e35094c473>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_time['Day']= pd.to_datetime(df['time_id']).dt.day\n",
      "<ipython-input-139-a8e35094c473>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_time['Month']= pd.to_datetime(df['time_id']).dt.month\n",
      "<ipython-input-139-a8e35094c473>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_time['Year']= pd.to_datetime(df['time_id']).dt.year\n",
      "<ipython-input-139-a8e35094c473>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_time.drop_duplicates(inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-23 14:45:32</td>\n",
       "      <td>14:45:32</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-23 14:45:31</td>\n",
       "      <td>14:45:31</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-04-23 14:45:30</td>\n",
       "      <td>14:45:30</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-04-23 14:45:28</td>\n",
       "      <td>14:45:28</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-23 14:45:26</td>\n",
       "      <td>14:45:26</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-04-22 14:18:31</td>\n",
       "      <td>14:18:31</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-04-22 23:55:45</td>\n",
       "      <td>23:55:45</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-04-22 23:56:03</td>\n",
       "      <td>23:56:03</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-04-21 22:00:00</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-04-21 18:27:00</td>\n",
       "      <td>18:27:00</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               time_id      Time  Day  Month  Year\n",
       "0  2021-04-23 14:45:32  14:45:32   23      4  2021\n",
       "2  2021-04-23 14:45:31  14:45:31   23      4  2021\n",
       "5  2021-04-23 14:45:30  14:45:30   23      4  2021\n",
       "7  2021-04-23 14:45:28  14:45:28   23      4  2021\n",
       "8  2021-04-23 14:45:26  14:45:26   23      4  2021\n",
       "..                 ...       ...  ...    ...   ...\n",
       "14 2021-04-22 14:18:31  14:18:31   22      4  2021\n",
       "15 2021-04-22 23:55:45  23:55:45   22      4  2021\n",
       "16 2021-04-22 23:56:03  23:56:03   22      4  2021\n",
       "18 2021-04-21 22:00:00  22:00:00   21      4  2021\n",
       "19 2021-04-21 18:27:00  18:27:00   21      4  2021\n",
       "\n",
       "[329 rows x 5 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time_dim(fact_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newspapers= Daily Mail, Extra, Variety, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Using cached PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymysql.connections.Connection object at 0x7fa39385ac70>\n"
     ]
    }
   ],
   "source": [
    "connection = pymysql.connect(host= '127.0.0.1',\n",
    "                            user = 'root', \n",
    "                            password = '',\n",
    "                            db= '',\n",
    "                            cursorclass = pymysql.cursors.DictCursor)\n",
    "\n",
    "print (connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite://', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3'], 'age': [23,45,34]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'User 1', 23), (1, 'User 2', 45), (2, 'User 3', 34)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df2.to_sql('test', con=engine)\n",
    "\n",
    "engine.execute(\"SELECT * FROM test\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery = \"CREATE TABLE tweets (retweet_count INT, text VARCHAR (1000), author VARCHAR (255), created_at datetime)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor= connection.cursor()\n",
    "connection.commit()\n",
    "sql2 = \"show tables;\" \n",
    "result = cursor.fetchone()\n",
    "\n",
    "cols = \"`,`\".join([str(i) for i in df5.columns.tolist()])\n",
    "\n",
    "# Insert DataFrame recrds one by one.\n",
    "for i,row in df5.iterrows():\n",
    "    sqlQuery = \"INSERT INTO `tweets` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "    cursor.execute(sqlQuery, tuple(row))\n",
    "\n",
    "    # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "    connection.commit()\n",
    "    \n",
    "sql = \"SELECT * FROM `tweets`\"\n",
    "cursor.execute(sql)\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for i in result:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
